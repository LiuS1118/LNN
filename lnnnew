import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split

class NeuralNetworkModel(tf.keras.Model):
    def __init__(self, layers, output_size, alpha=0.5):
        super(NeuralNetworkModel, self).__init__()
        self.alpha = alpha
        self.dense_layers = []

        # 使用 He 初始化器
        he_initializer = tf.keras.initializers.HeNormal()

        # 添加层并交替 BatchNormalization 和 Dropout
        for units in layers:
            self.dense_layers.append(Dense(units, activation=None, kernel_initializer=he_initializer))
            self.dense_layers.append(BatchNormalization())
            self.dense_layers.append(tf.keras.layers.Lambda(lambda x: self.alpha * x))
            self.dense_layers.append(Dropout(0.3))

        self.output_layer = Dense(output_size, kernel_initializer=he_initializer)

    def call(self, inputs, training=False):
        x = inputs
        for layer in self.dense_layers:
            if isinstance(layer, (BatchNormalization, Dropout)):
                x = layer(x, training=training)
            else:
                x = layer(x)
        return self.output_layer(x)

# Load datasets
df_X = pd.read_csv('cbc2.csv', index_col=False, keep_default_na=False)
X_matrix = df_X.iloc[0:498, 0:20].to_numpy().T

# 标准化数据
scaling_factor = 10
X_normalized = (X_matrix - np.mean(X_matrix)) / np.std(X_matrix)
X = tf.constant(X_normalized * scaling_factor, dtype=tf.float32)

# 读取目标数据
df_f = pd.read_csv('f2.csv', index_col=False, keep_default_na=False)
f_matrix = df_f.iloc[0:498, 0:20].to_numpy().T
f = tf.constant(f_matrix, dtype=tf.float32)

# Model initialization (no indentation issue)
model = NeuralNetworkModel(layers=[50, 5, 50, 50, 5, 50], output_size=498, alpha=0.5)
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.001,
    decay_steps=100,
    decay_rate=0.96,
    staircase=True
)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)

def compute_loss_and_grads(model, X, f, lambda_l1=0.000001, lambda_l2=0.000009):
    with tf.GradientTape() as tape:
        logits = model(X, training=True)
        mse_loss = tf.reduce_mean(tf.square(f - logits))
        l1_loss = tf.add_n([tf.reduce_sum(tf.abs(var)) for var in model.trainable_variables])
        l2_loss = tf.add_n([tf.reduce_sum(tf.square(var)) for var in model.trainable_variables])
        loss_value = mse_loss + lambda_l1 * l1_loss + lambda_l2 * l2_loss
        grads = tape.gradient(loss_value, model.trainable_variables)
    return loss_value, grads

def train_and_evaluate_mse(model, X, f, num_epochs=5000, lambda_l1=0.000001, lambda_l2=0.000009):
    loss_values = []
    max_loss = -np.inf
    min_loss = np.inf
    
    for epoch in range(num_epochs):
        # 计算当前批次的损失和梯度
        loss_value, grads = compute_loss_and_grads(model, X, f)
        # 更新模型权重
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
        
        # 保存当前损失
        loss_values.append(loss_value.numpy())
        avg_loss = np.mean(loss_values)

        # 更新最大和最小损失
        max_loss = max(max_loss, loss_value.numpy())
        min_loss = min(min_loss, loss_value.numpy())

        # 输出当前 epoch 的损失
        print(f'Epoch {epoch + 1}: Avg MSE Loss = {avg_loss:.4f}, Max Loss = {max_loss:.4f}, Min Loss = {min_loss:.4f}')

    return loss_values, max_loss, min_loss

# 训练并评估模型
loss_values, max_loss, min_loss = train_and_evaluate_mse(model, X, f)

# 绘制损失曲线
plt.figure()
plt.plot(range(len(loss_values)), loss_values)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss Curve')
plt.show()
print(f'Max Loss during training: {max_loss:.4f}')
print(f'Min Loss during training: {min_loss:.4f}')

# 输出最终的预测
hat_f = model(X, training=False).numpy()
print("Final Solution:")
print(hat_f)

# 计算均方误差 (MSE)
mse = np.mean(np.square(f.numpy() - hat_f))
print("\nMean Squared Error (MSE):")
print(mse)

# 输出最终的权重矩阵
# for var in model.trainable_variables:
#     print(f"Name: {var.name}, Shape: {var.shape}")
#     print(var.numpy())

# 对每个列向量求和
column_sums = np.sum(hat_f, axis=0)
print("\nColumn Sums:")
print(column_sums)

# 分类
categories = {
   "[0, 5)": [],
   "[5, 10)": [],
   "[10, 15)": [],
   "[15, 20]": []
}

for i, sum_value in enumerate(column_sums):
    if 0 <= sum_value < 5:
        categories["[0, 5)"].append(i)
    elif 5 <= sum_value < 10:
        categories["[5, 10)"].append(i)
    elif 10 <= sum_value <= 15:
        categories["[10, 15)"].append(i)
    elif 15 <= sum_value <= 20:
        categories["[15, 20]"].append(i)

# 打印分类结果
print("\nRow Indices by Category:")
for category, indices in categories.items():
    print(f"{category}: {indices}")

# 计算每个类别的数量
category_counts = [len(indices) for indices in categories.values()]

# 定义饼图的颜色
colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']

# 绘制饼图并改进美学效果
fig, ax = plt.subplots(figsize=(10, 8))
wedges, texts, autotexts = ax.pie(
    category_counts,
    labels=None,
    autopct='%1.1f%%',
    startangle=140,
    colors=colors
)

# 调整自动文本的字体大小来放大百分比字号
for autotext in autotexts:
    autotext.set_fontsize(18)

# 添加图例（可选）
ax.legend(
    wedges,
    categories.keys(),
    title="Categories",
    loc="center left",
    bbox_to_anchor=(1, 0, 0.5, 1),
    fontsize=12
)

plt.show()
