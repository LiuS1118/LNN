import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
import numpy as np
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt  

class NeuralNetworkModel(tf.keras.Model):
    def __init__(self, layers, output_size, alpha=0.5):
        super(NeuralNetworkModel, self).__init__()
        self.alpha = alpha
        self.dense_layers = []

        # 使用 He 初始化器
        he_initializer = tf.keras.initializers.HeNormal()  # 或者使用 HeUniform()

        # 添加层并交替 BatchNormalization 和 Dropout
        for units in layers:
            self.dense_layers.append(Dense(units, activation=None, kernel_initializer=he_initializer))  # 添加全连接层
            self.dense_layers.append(BatchNormalization())  # 添加 BatchNormalization 层
            self.dense_layers.append(tf.keras.layers.Lambda(lambda x: self.alpha * x))  # 添加缩放层
            self.dense_layers.append(Dropout(0.3))  # 添加 Dropout 层

        self.output_layer = Dense(output_size, kernel_initializer=he_initializer)  # 添加输出层

    def call(self, inputs, training=False):
        x = inputs
        for layer in self.dense_layers:
            if isinstance(layer, (BatchNormalization, Dropout)):
                x = layer(x, training=training)  # 训练时启用 Dropout 和 BatchNormalization
            else:
                x = layer(x)  # 其他层直接调用
        return self.output_layer(x)  # 返回输出层的结果


def compute_loss_and_grads(model, X, f, lambda_l1=0.000001, lambda_l2=0.000009):
    with tf.GradientTape() as tape:
        logits = model(X, training=True)
        mse_loss = tf.reduce_mean(tf.square(f - logits))
        l1_loss = tf.add_n([tf.reduce_sum(tf.abs(var)) for var in model.trainable_variables])
        l2_loss = tf.add_n([tf.reduce_sum(tf.square(var)) for var in model.trainable_variables])
        loss_value = mse_loss + lambda_l1 * l1_loss + lambda_l2 * l2_loss
        grads = tape.gradient(loss_value, model.trainable_variables)
    return loss_value, grads

def train_and_evaluate_mse(model, optimizer, X_train, f_train, X_val, f_val, num_epochs=5000):
    train_losses = []
    val_losses = []
    
    for epoch in range(num_epochs):
        loss_value, grads = compute_loss_and_grads(model, X_train, f_train)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        train_losses.append(loss_value.numpy())

        val_logits = model(X_val, training=False)
        val_loss = tf.reduce_mean(tf.square(f_val - val_logits))
        val_losses.append(val_loss.numpy())

        # 监控训练过程
        if (epoch + 1) % 50 == 0:
            print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {loss_value.numpy():.4f}, Val Loss: {val_loss.numpy():.4f}")

    return train_losses, val_losses

# Load datasets

df_X = pd.read_csv('cbc2.csv', index_col=False, keep_default_na=False)
X_matrix = df_X.iloc[0:498, 0:20].to_numpy()

# 标准化数据
scaling_factor = 10
X_normalized = (X_matrix - np.mean(X_matrix)) / np.std(X_matrix)
X = tf.constant(X_normalized * scaling_factor, dtype=tf.float32)

# 读取目标数据
df_f = pd.read_csv('f2.csv', index_col=False, keep_default_na=False)
f_matrix = df_f.iloc[0:498, 0:20].to_numpy()
f = tf.constant(f_matrix, dtype=tf.float32)


k_folds = 15
kf = KFold(n_splits=k_folds, shuffle=True, random_state=55)

# Initialize lists for averaging losses
epoch_wise_train_losses = []
epoch_wise_val_losses = []
epoch_wise_train_mse = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"Fold {fold + 1}/{k_folds}")
    
    # Print indices instead of the entire dataset
    print(f"Training indices for Fold {fold + 1}: {train_idx}")
    print(f"Validation indices for Fold {fold + 1}: {val_idx}")

    X_train, X_val = tf.gather(X, train_idx), tf.gather(X, val_idx)
    f_train, f_val = tf.gather(f, train_idx), tf.gather(f, val_idx)
    #Print the training and validation datasets and their shapes
    print(f"Training set (X_train) for Fold {fold + 1}:\n{X_train.numpy()}")
    print(f"Training target (f_train) for Fold {fold + 1}:\n{f_train.numpy()}")
    print(f"Validation set (X_val) for Fold {fold + 1}:\n{X_val.numpy()}")
    print(f"Validation target (f_val) for Fold {fold + 1}:\n{f_val.numpy()}")

    # Print the shapes of the datasets
    print(f"Shape of X_train for Fold {fold + 1}: {X_train.shape}")
    print(f"Shape of f_train for Fold {fold + 1}: {f_train.shape}")
    print(f"Shape of X_val for Fold {fold + 1}: {X_val.shape}")
    print(f"Shape of f_val for Fold {fold + 1}: {f_val.shape}")
    
    
    model = NeuralNetworkModel(layers=[50, 5, 50, 50, 5, 50], output_size=20, alpha=0.5)  # 增加层的神经元数量
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=0.001,
        decay_steps=100,
        decay_rate=0.96,
        staircase=True
    )
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)

    train_losses, val_losses = train_and_evaluate_mse(model, optimizer, X_train, f_train, X_val, f_val, num_epochs=5000)

    val_logits = model(X_val, training=False)
    val_loss = tf.reduce_mean(tf.square(f_val - val_logits))
    print(f"Validation MSE Loss for Fold {fold + 1}: {val_loss.numpy():.4f}")

    train_mse_logits = model(X_train, training=False)
    train_mse = tf.reduce_mean(tf.square(f_train - train_mse_logits))
    print(f"Train MSE Loss for Fold {fold + 1}: {train_mse.numpy():.4f}")


    # Store epoch-wise losses
    epoch_wise_train_losses.append(train_losses)
    epoch_wise_val_losses.append(val_losses)

# Average the losses across folds
avg_train_losses = np.mean(epoch_wise_train_losses, axis=0)
avg_val_losses = np.mean(epoch_wise_val_losses, axis=0)

# Plot average training and validation loss
plt.figure(figsize=(12, 6))
plt.plot(avg_train_losses, label='Average Train Loss', color='blue')
plt.plot(avg_val_losses, label='Average Validation Loss', color='orange')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error Loss')
plt.title('Average Training and Validation Loss Across Folds')
plt.legend()
plt.grid()
plt.show()

